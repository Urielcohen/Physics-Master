{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef1d5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch import Tensor\n",
    "from numbers import Number\n",
    "from typing import Union\n",
    "\n",
    "from torch.nn import Sequential\n",
    "import torchvision.transforms as tvts\n",
    "from torch.utils.data import IterableDataset, IterDataPipe\n",
    "from torchdata.datapipes.iter import Mapper\n",
    "\n",
    "%run DDRM_sampling.ipynb\n",
    "\n",
    "\n",
    "class GalaxyImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Image dataset. Automatically detects file extensions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir=\"/project/undark/galaxy_zoo_jpgs/\",\n",
    "        to_gray=True,\n",
    "        n_pix=None,\n",
    "        dtype=torch.float32,\n",
    "        n_max=None,\n",
    "    ):\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "        exts = [\"jpg\", \"jpeg\", \"png\", \"npy\"]\n",
    "        self.fnames = []\n",
    "        for ext in exts:\n",
    "            self.fnames.extend(glob.glob(os.path.join(root_dir, f\"*.{ext}\")))\n",
    "        self.fnames = list(sorted(self.fnames))\n",
    "\n",
    "        self.n_images = len(self.fnames)\n",
    "        if n_max is not None:\n",
    "            self.n_images = min(self.n_images, n_max)\n",
    "\n",
    "        self.to_gray = to_gray\n",
    "        self.n_pix = n_pix\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.fnames[idx]\n",
    "        ext = os.path.splitext(fname)[1][1:]\n",
    "        if ext in [\"jpg\", \"jpeg\", \"png\"]:\n",
    "            img = io.imread(fname)\n",
    "        elif ext == \"npy\":\n",
    "            img = np.load(fname)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"cannot load files with the extension {ext}\")\n",
    "\n",
    "        if idx >= self.n_images or idx < 0:\n",
    "            raise IndexError()\n",
    "\n",
    "        # Put channel last\n",
    "        if img.shape[0] == 3:\n",
    "            img = np.moveaxis(img, 0, -1)\n",
    "\n",
    "        if self.n_pix is not None:\n",
    "            img = resize(img, (self.n_pix, self.n_pix))\n",
    "\n",
    "        if self.to_gray:\n",
    "            assert img.shape[-1] == 3\n",
    "            img = rgb2gray(img)\n",
    "            return torch.as_tensor(img, dtype=self.dtype)\n",
    "        else:\n",
    "            return torch.as_tensor(img, dtype=self.dtype).movedim(-1, -3)\n",
    "\n",
    "\n",
    "class Normalize(torch.nn.Module):\n",
    "    def __init__(self, maximum: Union[Tensor, Number], minimum: Union[Tensor, Number] = 0, clip=True):\n",
    "        super().__init__()\n",
    "        self.max = maximum\n",
    "        self.min = minimum\n",
    "        self.clip = clip\n",
    "\n",
    "    def forward(self, t: Tensor):\n",
    "        res = (t - self.min) / self.max\n",
    "        return res.clip_(0, 1) if self.clip else res\n",
    "\n",
    "\n",
    "class ProbesDataset(IterableDataset[Tensor]):\n",
    "    def __init__(\n",
    "        self, files: IterDataPipe, size: int, loadfunc=torch.load,\n",
    "        max_rotation=180, scale: Union[Number, tuple[Number, Number]]=1.2, max_translation=0.1,\n",
    "        interp=tvts.InterpolationMode.BILINEAR, norm=5.5, clip=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A dataset that loads galaxy images from a folder.\n",
    "        Applies data augmentation using random rotation, scale, translation,\n",
    "        and flipping, then crops the image to a desired size and normalises it.\n",
    "        Parameters\n",
    "        ----------\n",
    "        folder\n",
    "            location of the data: passed to `FileLister`\n",
    "        size\n",
    "            size of output images\n",
    "        loadfunc\n",
    "            function used to load file into `Tensor`. You may wish to pass\n",
    "            ``partial(torch.load, map_device=DEVICE)`` to load directly on\n",
    "            ``DEVICE``\n",
    "        max_rotation\n",
    "            maximum rotation (degrees) applied during augmentation\n",
    "        scale\n",
    "            scale passed to `RandomAffine` for augmentation. If `int`,\n",
    "            interpreted as ``(1/scale, scale)``.\n",
    "        max_translation\n",
    "            maximum translation (as fraction of *original* image size) applied\n",
    "            during augmentation (passed to `RandomAffine`)\n",
    "        interp\n",
    "            interpolation type passed to `RandomAffine`\n",
    "        norm\n",
    "            normalise images by this value\n",
    "        clip\n",
    "            clip values to the range ``(0, norm)`` (remapped to ``(0, 1)``)\n",
    "        kwargs\n",
    "            extra keyword arguments passed to `FileLister`\n",
    "        \"\"\"\n",
    "        self.loader = Mapper(files, loadfunc)\n",
    "        self.augmentation = Sequential(\n",
    "            tvts.RandomAffine(\n",
    "                degrees=max_rotation, scale=(1/scale, scale) if isinstance(scale, Number) else scale,\n",
    "                translate=2*(max_translation,),\n",
    "                interpolation=interp),\n",
    "            tvts.CenterCrop(size),\n",
    "            tvts.RandomHorizontalFlip(), tvts.RandomVerticalFlip(),\n",
    "            Normalize(norm, clip=clip)\n",
    "        )\n",
    "\n",
    "    def __iter__(self):\n",
    "        return map(self.augmentation, self.loader)\n",
    "    \n",
    "    \n",
    "class DDPMDataset(LensingDDPM, IterableDataset[Tensor]):\n",
    "    def __init__(self, diffusion: GaussianDiffusion, nbatch: int=None, clip=True, multi_channel=True, *, show_progress=True):\n",
    "        \"\"\"\n",
    "        A dataset that samples galaxy images from a pre-trained DDPM model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        diffusion\n",
    "            The pre-trained DDPM model.\n",
    "        nbatch\n",
    "            The number of images to sample in each iteration. Passing `None`\n",
    "            disables batching and yields a single image with shape (C, H, W).\n",
    "        clip\n",
    "            Whether to clip to [-1, 1] while sampling. Note that the output is\n",
    "            always re-normalised to [0, 1] (but not further clipped).\n",
    "        multi_channel\n",
    "            Whether to sample multi-channel images. Setting this to `False` is\n",
    "            undefined for the moment.\n",
    "        show_progress\n",
    "            Whether to show a progress bar for the denoising steps.\n",
    "        \"\"\"\n",
    "        super().__init__(diffusion=diffusion, multi_channel=multi_channel, show_progress=show_progress)\n",
    "        self.nbatch = nbatch\n",
    "        self.clip = clip\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            res = self.prior(1 if self.nbatch is None else self.nbatch, self.clip).flip(-3).add_(1).mul_(0.5)\n",
    "            yield res.squeeze(-4) if self.nbatch is None else res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "819600a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "            \n",
    "class DDRMDataset(LensingDDRM, IterableDataset[Tensor]):\n",
    "    def __init__(self, diffusion: GaussianDiffusion,\n",
    "                 Hb: Tensor, H: Tensor = None, U: Tensor = None, S: Tensor = None, Vh: Tensor = None,\n",
    "                 nbatch: int=1, clip=True, multi_channel=True, *, show_progress=True,\n",
    "                 y: Tensor = None, sigma_y=1, eta=0.03, eta_b=0.85, skip=1, skip_type=\"linear\"):\n",
    "        \"\"\"\n",
    "        A dataset that samples galaxy images from a DDRM model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        diffusion\n",
    "            The pre-trained DDPM model.\n",
    "        Hb, H, U, S, Vh\n",
    "            SVD matrices.\n",
    "        nbatch\n",
    "            The number of images to sample in each iteration. Passing `None`\n",
    "            disables batching and yields a single image with shape (C, H, W).\n",
    "        clip\n",
    "            Whether to clip to [-1, 1] while sampling. Note that the output is\n",
    "            always re-normalised to [0, 1] (but not further clipped).\n",
    "        multi_channel\n",
    "            Whether to sample multi-channel images. Setting this to `False` is\n",
    "            undefined for the moment.\n",
    "        show_progress\n",
    "            Whether to show a progress bar for the denoising steps.\n",
    "        y\n",
    "            Noisy observation on which to condition.\n",
    "        sigma_y\n",
    "            Noise level.\n",
    "        eta\n",
    "            eta hyperparmater for DDRM.\n",
    "        eta_b\n",
    "            eta_b hyperparmater for DDRM.\n",
    "        skip\n",
    "            How many steps to skip in the chain while sampling to accelerate DDRM.\n",
    "        skip_type\n",
    "            How to skip steps in the chain while sampling to accelerate DDRM. Options: \"linear\", \"quad\"\n",
    "        \"\"\"\n",
    "        super().__init__(diffusion=diffusion, Hb=Hb, H=H, U=U, S=S, Vh=Vh, multi_channel=multi_channel, show_progress=show_progress)\n",
    "        self.nbatch = nbatch\n",
    "        self.clip = clip\n",
    "        self.y = y\n",
    "        self.sigma_y = sigma_y\n",
    "        self.eta = eta\n",
    "        self.eta_b = eta_b\n",
    "        self.skip = skip\n",
    "        self.skip_type = skip_type        \n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "#             print(self.y.shape)\n",
    "#             print(self.H.shape)\n",
    "            #print(self.H.sum(1).sum(1).shape)\n",
    "            Y = 2*self.y - self.H.sum(1) # normalization\n",
    "            \n",
    "            res = self.sample(\n",
    "                y=Y.expand(self.nbatch, *Y.shape[-3 if self.multi_channel else -2:]),\n",
    "                sigma_y=2*self.sigma_y, skip=self.skip, skip_type=self.skip_type, clip=self.clip, eta=self.eta, eta_b=self.eta_b\n",
    "            ).add_(1).mul_(0.5).unflatten(-1, self.src_shape)\n",
    "            yield res.squeeze(-4) if self.nbatch==1 else res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
